{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STI_ver3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMg94KWnGU1oy28xqMxBI8m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hatahataDev/SDGsproject/blob/master/STI_ver3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpuoxjwS27Jx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#上から順に実行していってください．\n",
        "#最後のコードは１時間ほどかかります．すべての処理が終わると予測データのＣＳＶが出力されます．\n",
        "\n",
        "from google.colab import drive # グーグルドライブをマウントさせるコード。\n",
        "drive.mount('/content/drive')\n",
        "data_dir=\"drive/My Drive/*/STI2_v2.csv\" # CSVファイルのパス　学習データ\n",
        "predict_file=\"drive/My Drive/*/4th STI-Gigaku.csv\" # CSVファイル 予測したいデータ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSMpvH643DeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://qiita.com/shimayu22/items/4fc6d01a4bf0eef909a0 参考記事\n",
        "# https://qiita.com/shimayu22/items/4fc6d01a4bf0eef909a0\n",
        "# ネット回線ないとコード動きません。\n",
        "\n",
        "def data_change(sdgs_input_data):\n",
        "  import os\n",
        "  import csv\n",
        "  import nltk\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  nltk.download('punkt') # nltkを使うためのソフト群\n",
        "  nltk.download('tagsets')\n",
        "  nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "  csv_file = open(data_dir,'r') # 学習データ\n",
        "  p_n_list = sdgs_input_data\n",
        "  # p_csv_file= open(predict_dir,\"r\") # 予測したいデータ\n",
        "  \"\"\"\n",
        "  csvファイルは左から目標番号、タイトル、アブスト、結論の順で入れること。\n",
        "  \"\"\"\n",
        "  p_list,t_list,a_list = [],[],[]\n",
        "  # c_list= []\n",
        "\n",
        "  delete= [\"a\",\"%\",\"of\",\"in\",\"on\",\"to\",\"for\",\"are\",\"at\",\"[\",\"]\"] # 関連性のない語を集めた配列\n",
        "\n",
        "  # p_n_list=[]#予測したいデータのテキストバッグ\n",
        "\n",
        "  # for row in csv.reader(p_csv_file):\n",
        "  # p_n_list.append(row[1])\n",
        "  # p_n_list.append(row[2])\n",
        "\n",
        "  x, y = [], []\n",
        "  count = 0\n",
        "\n",
        "  for row in csv.reader(csv_file):\n",
        "    # 以下リストを作成\n",
        "      p_list.append(row[0]) # 取得したい列番号を指定（0始まり）#目標番号をすべて取得\n",
        "      t_list.append(row[1]) # タイトルをすべて取得\n",
        "      a_list.append(row[2]) # アブストをすべて取得\n",
        "      # c_list.append(row[3]) # 結論も入れる\n",
        "\n",
        "  del p_list[0] # 0番目を削除\n",
        "  del t_list[0] # 0番目は削除\n",
        "  del a_list[0] # 0番目は削除。これで_list[i]でそれぞれの列を抜き出すことができる。\n",
        "  # del c_list[0]\n",
        "\n",
        "  # 以下３行　予測したいデータの形態素解析\n",
        "  # p_n_list=p_n_list[0]+p_n_list[1] \n",
        "  p_n_words = nltk.word_tokenize(p_n_list)\n",
        "  p_n_wards=nltk.pos_tag(p_n_words)# [(\"単語\",\"品詞\")....]の２次元リストをここで生成\n",
        "\n",
        "\n",
        "  p_result = [] # 名詞の数字配列を入れるp_resultを定義\n",
        "  p_N_dic = [] # 名詞のstring型配列を定義\n",
        "  # 以下名詞を抜き出しN_dicに入れ込み\n",
        "  for word,part in p_n_wards: # wordに単語、partに品詞が入っている→このforはタイトル\n",
        "      if part==\"NN\" or part==\"NNP\" or part==\"VBP\": # 以下if文は名詞のみをN_dicというリストに格納\n",
        "        if not word.lower() in delete: # さらにdeleteリストに入っている意味のない名詞を削除\n",
        "          sword=word.lower()\n",
        "          p_N_dic.append(sword)\n",
        "\n",
        "  Cdic=[] # 各ゴールの単語をすべて入れる。推薦単語用.Cdic[1]→1番目の単語すべてを返す\n",
        "\n",
        "  for i in range(18): # それぞれのゴールの中身を内包した収納配列を生成\n",
        "    a=[]\n",
        "    Cdic.append(a) # [[],[],[]......18個]この中に単語をすべて入れていく\n",
        "\n",
        "  dic={} # 辞書で定義\n",
        "  for i,p in enumerate(p_list): # p_listのiをカウンタ変数,ｐを正解ラベル\n",
        "    j=i+1 # p_list,a_listなどは0番目が存在しないため無理やりjをカウンター変数に置き換え\n",
        "    if j>=len(p_list): # +1してるから最後の一つだけ存在しないjが入るためここで分岐処理　t_list[j_max]は存在しないためエラー\n",
        "      break\n",
        "    N_dic=[] # ここで名詞リストは初期化\n",
        "    # print(t_list[i])\n",
        "    t_string = t_list[i]\n",
        "    t_words = nltk.word_tokenize(t_string)\n",
        "    t_wards=nltk.pos_tag(t_words)# [(\"単語\",\"品詞\")....]の２次元リストをここで生成\n",
        "    # print(t_wards)\n",
        "    a_string = a_list[i]\n",
        "    a_words = nltk.word_tokenize(a_string)\n",
        "    a_wards=nltk.pos_tag(a_words)\n",
        "    \"\"\"\n",
        "    c_string = c_list[j]\n",
        "    c_words = nltk.word_tokenize(c_string)\n",
        "    c_wards=nltk.pos_tag(c_words)\n",
        "    \"\"\"\n",
        "    for word,part in t_wards: # wordに単語、partに品詞が入っている→このforはタイトル\n",
        "      if part==\"NN\" or part==\"NNP\" or part==\"VBP\" or part==\"NNS\" or part==\"NNPS\": # 以下if文は名詞のみをN_dicというリストに格納\n",
        "        if not word.lower() in delete: #さらにdeleteリストに入っている意味のない名詞を削除\n",
        "          sword=word.lower() # 小文字に変換\n",
        "          N_dic.append(sword)\n",
        "          Cdic[int(p)].extend([sword]) # swordはstr型なので無理やりリストに変換 extendはlist型しか入らない\n",
        "    for word,part in a_wards: # wordに単語、partに品詞が入っている→このforはアブスト\n",
        "      if part==\"NN\" or part==\"NNP\" or part==\"VBP\" or part==\"NNS\" or part==\"NNPS\": # 以下if文は名詞のみをN_dicというリストに格納\n",
        "        if not word.lower() in delete:\n",
        "          sword=word.lower()\n",
        "          N_dic.append(sword)\n",
        "          Cdic[int(p)].extend([sword]) # pはstrなのでキャスト\n",
        "    \"\"\"\n",
        "    for word,part in c_wards: #wordに単語、partに品詞が入っている→このforはアブスト\n",
        "      if part==\"NN\": #以下if文は名詞のみをN_dicというリストに格納\n",
        "        N_dic.append(word)\n",
        "      if part==\"NNP\":\n",
        "        N_dic.append(word)\n",
        "    \"\"\"\n",
        "  # print(N_dic)# すべてN_dicにまとめてしまう。\n",
        "\n",
        "    result=[]\n",
        "    for word in N_dic:\n",
        "      word = word.strip()  \n",
        "      if word == \"\": continue\n",
        "      if not word in dic: # 未登録の場合\n",
        "        dic[word] = count  # count の数字で辞書に登録　補足：辞書型はwordとkeyの二つの情報を入れ込むことができる.これでNNに入れれるようにしていく\n",
        "        num = count\n",
        "        count +=1\n",
        "      # print(num,word)  # 数字と単語を表示\n",
        "      else:\n",
        "        num=dic[word] # 数字を辞書で調べる\n",
        "      result.append(num)  # リストに数字を追加 ここで名詞の塊を数値化している\n",
        "    x.append(result)  # リストを配列 ｘ に追加 x = [ [ 10, 11, 12, 3, 8, 13,  … 38 ] , [ 39, 40, 12, 16, 19, … ,56 ] , ….  ,  [ 1200, 1504, 3, 15, … 3300 ] ] \n",
        "    p_p=int(p)  # pは文字型で入っているのでキャスト\n",
        "    y.append(p_p)  # 正解ラベルを配列 y に追加 y = [ 0, 0, 0, 0, … ,8, 8, 8, 8 ] \n",
        "\n",
        "  # 以下のfor 予測したいデータの名詞配列の数値化 or 辞書の生成\n",
        "  for word in p_N_dic:\n",
        "      word = word.strip()  \n",
        "      if word == \"\": continue\n",
        "      if not word in dic: # 未登録の場合\n",
        "        dic[word] = count  # count の数字で辞書に登録　補足：辞書型はwordとkeyの二つの情報を入れ込むことができる.これでNNに入れれるようにしていく\n",
        "        num = count\n",
        "        count +=1\n",
        "      # print(num,word)  # 数字と単語を表示\n",
        "      else:\n",
        "        num=dic[word] # 数字を辞書で調べる\n",
        "      p_result.append(num)  # リストに数字を追加 ここで名詞の塊を数値化している\n",
        "\n",
        "  return x,y,p_result,dic \n",
        "  #print(x)\n",
        "  #print(y)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQaVOclB3Di2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#学習機\n",
        "def learning_predicting(x,y,p_result,dic):\n",
        "  import numpy as np\n",
        "  import keras\n",
        "  import glob\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Dense, Dropout, Activation\n",
        "  from keras.preprocessing.text import Tokenizer\n",
        "  from sklearn.model_selection import train_test_split ### 追加\n",
        "  from collections import Counter #カウントするメソッドを使うためのモジュール\n",
        "\n",
        "  max_words = len(dic)\n",
        "  batch_size = 2\n",
        "  epochs = 10\n",
        "\n",
        "  #x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.0025, random_state=111)\n",
        "  x_train=x\n",
        "  y_train=y\n",
        "\n",
        "  print(len(x_train), 'train sequences')\n",
        "  #print(len(x_test), 'test sequences')\n",
        "  \n",
        "  num_classes = np.max(y_train) +1\n",
        "  print(num_classes, 'classes')\n",
        "  \n",
        "  print('Vectorizing sequence data...')\n",
        "  tokenizer = Tokenizer(num_words=max_words)\n",
        "  x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "  #x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "\n",
        "  print('x_train shape:', x_train.shape)\n",
        "  #print('x_test shape:', x_test.shape)\n",
        "  \n",
        "  print('Convert class vector to binary class matrix '\n",
        "  '(for use with categorical_crossentropy)')\n",
        "  y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "  #y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "  print('y_train shape:', y_train.shape)\n",
        "  #print('y_test shape:', y_test.shape)\n",
        "\n",
        "  # modelのインスタンス生成\n",
        "  print('Building model...')\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Dense(206, input_shape=(max_words,)))\n",
        "  model.add(Activation(\"sigmoid\"))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))#２層ニューラルネットワーク\n",
        "\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']) \n",
        "  \n",
        "  history = model.fit(x_train, y_train,\n",
        "  batch_size=batch_size,\n",
        "  epochs=epochs,\n",
        "  verbose=0)\n",
        "  #score = model.evaluate(x_test, y_test,\n",
        "  #batch_size=batch_size, verbose=1)\n",
        "\n",
        "  #pre=model.predict( x_test, batch_size=batch_size, verbose=1) #それぞれの正解ラベルに対する確率を返す\n",
        "\n",
        "  #予測データのバイナリー化\n",
        "  def binary(result):\n",
        "    arr=np.zeros(len(dic)) #辞書数分のバイナリー配列を生成。すべて０で最初は生成\n",
        "    for f in result: #予測したい文脈の名詞の番号のところに１を立てる。\n",
        "      arr[f]=1\n",
        "\n",
        "    arr=np.array(arr) #numpy配列に変換\n",
        "    arr.reshape(1,len(dic))\n",
        "    arrd=np.stack([arr, arr]) #無理やり配列を拡張。model.predictの引数に拡張しないと入れられないため。(2,len(dic))の配列にする\n",
        "    return arrd\n",
        "  #print(arrd.shape)\n",
        "\n",
        "  #print('Test score:', score[0])\n",
        "  #print('Test accuracy:', score[1])\n",
        "  #print(pre1)\n",
        "\n",
        "  #出力関数\n",
        "  def result(pre):\n",
        "    result= []\n",
        "    for i,c in enumerate(pre[0]): #２次元配列を生成\n",
        "      hum=[c,i]\n",
        "      result.append(hum)\n",
        "    result2=sorted(result,reverse=True) #大きい順にソート\n",
        "\n",
        "    j=0\n",
        "    for k,l in enumerate(result2):\n",
        "      if l[1]==0:\n",
        "        j=1\n",
        "        continue\n",
        "      if j==1:\n",
        "        print(str(k)+\"番目に関連が高いのは\"+str(l[1])+\"の目標で,関連度は\"+str(l[0]*100)+\"％です。\")\n",
        "      else:\n",
        "        print(str(k+1)+\"番目に関連が高いのは\"+str(l[1])+\"の目標で,関連度は\"+str(l[0]*100)+\"％です\")\n",
        "    return result2\n",
        "  bin_pre=binary(p_result)\n",
        "  pre1=model.predict( bin_pre, batch_size=batch_size, verbose=1)\n",
        "  current_result=result(pre1)\n",
        "  return pre1[0]#current_result\n",
        "\n",
        "  #推薦単語アルゴリズム\n",
        "  def renumbering(new_word,p_result): #推薦単語をすべて包括した場合の予測 任意の単語を追加して単語を数値化\n",
        "    jaka=[]\n",
        "    for word in new_word:\n",
        "        word = word.strip()  \n",
        "        if word == \"\": continue\n",
        "        if not word in dic: # 未登録の場合\n",
        "          dic[word] = count  # count の数字で辞書に登録　補足：辞書型はwordとkeyの二つの情報を入れ込むことができる.これでNNに入れれるようにしていく\n",
        "          num = count\n",
        "          count +=1\n",
        "        #print(num,word)  # 数字と単語を表示\n",
        "        else:\n",
        "          num=dic[word] # 数字を辞書で調べる\n",
        "          jaka.append(num)  # リストに数字を追加 ここで名詞の塊を数値化している\n",
        "    return p_result+jaka\n",
        "\n",
        "  \"\"\"\n",
        "  G=Counter(Cdic[current_result[0][1]]).most_common() #それぞれのゴール単語を包括した辞書を予測論文の１番関連度が高いゴールの単語を多い順に返してくれる。ここで型をリストに変えてくれる\n",
        "  print(G[0])\n",
        "  new_words=[] #表示させる推薦単語\n",
        "  for i,k in enumerate(G):\n",
        "    if i==10: #上位の単語をいくつ調べるかの設定。\n",
        "      break\n",
        "    if k[0] in p_N_dic: #その論文に推薦単語がある場合処理をスキップ\n",
        "      continue\n",
        "    print(\"他に包括したほうが良い単語は\"+k[0]+\"です。\\n\") \n",
        "    new_words.append(k[0])\n",
        "  #G[0][0] [出現単語][出現回数]\n",
        "\n",
        "  new_predict=renumbering(new_words,p_result)\n",
        "  repredict=binary(new_predict) #2値化\n",
        "  pre2=model.predict( repredict, batch_size=batch_size, verbose=1) #予測データ取得\n",
        "  print(\"上記の単語を包括した場合の関連度は以下の様になります。\")\n",
        "  result(pre2) #出力\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  print(\"その他のゴールも包括するには以下のような単語を推薦します\")\n",
        "  for i in range(18):\n",
        "    if i==0:\n",
        "      continue\n",
        "    if i==current_result[0][1]:\n",
        "      continue\n",
        "    G=Counter(Cdic[i]).most_common()\n",
        "    print(\"Goal\"+str(i)+\"⇩\")\n",
        "    for i,k in enumerate(G):\n",
        "      if i==10: #上位の単語をいくつ調べるかの設定。\n",
        "        break\n",
        "      if k[0] in p_N_dic: #その論文に推薦単語がある場合処理をスキップ\n",
        "        continue\n",
        "      print(\"他に包括したほうが良い単語は\"+k[0]+\"です。\\n\") \n",
        "  \"\"\"\n",
        "  # Plot accuracy\n",
        "  \"\"\"\n",
        "  import matplotlib.pyplot as plt\n",
        "  acc = history.history[\"acc\"]\n",
        "  val_acc = history.history[\"val_acc\"]\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "  \n",
        "  plt.plot(epochs, acc, \"bo\", label = \"Training acc\" )\n",
        "  plt.plot(epochs, val_acc, \"b\", label = \"Validation acc\")\n",
        "  plt.title(\"Training and Validation accuracy\")\n",
        "  plt.legend()\n",
        "  plt.savefig(\"acc.png\")\n",
        "  plt.close()\n",
        "  \n",
        "  ### plot Confusion Matrix\n",
        "  import pandas as pd\n",
        "  import seaborn as sn\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "  \n",
        "  def print_cmx(y_true, y_pred):\n",
        "    labels = sorted(list(set(y_true)))\n",
        "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "  \n",
        "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
        "  \n",
        "    plt.figure(figsize = (10,7))\n",
        "    sn.heatmap(df_cmx, annot=True, fmt=\"d\") ### ヒートマップの表示仕様\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"predict_classes\")\n",
        "    plt.ylabel(\"true_classes\")\n",
        "    plt.savefig(\"c_matrix.png\")\n",
        "    plt.close()\n",
        "\n",
        "  predict_classes = model.predict_classes(x_test[1:10000,], batch_size=32) ### 予測したラベルを取得\n",
        "  true_classes = np.argmax(y_test[1:10000],1) ### 実際のラベルを取得\n",
        "  print(confusion_matrix(true_classes, predict_classes))\n",
        "  print_cmx(true_classes, predict_classes)\n",
        "  \"\"\"\n",
        "  #[135, 554, 1126, 160, 2992, 4962, 4963, 2493, 4791, 554]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szM5HdY-3Dk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#データの予測orファイルへの書き込み\n",
        "import csv\n",
        "\n",
        "csv_file = open(predict_file,'r') # 学習データ\n",
        "p_list=[]\n",
        "t_list=[]\n",
        "a_list=[]\n",
        "#c_list=[]\n",
        "for row in csv.reader(csv_file):\n",
        "    # 以下リストを作成\n",
        "      p_list.append(row[0]) # 取得したい列番号を指定（0始まり）#目標番号をすべて取得\n",
        "      t_list.append(row[2]) # タイトルをすべて取得\n",
        "      a_list.append(row[3]) # アブストをすべて取得\n",
        "      #c_list.append(row[3]) # 結論も入れる #追加の行\n",
        "\n",
        "csv_file.close()\n",
        "del p_list[0] # 0番目を削除\n",
        "del t_list[0] # 0番目は削除\n",
        "del a_list[0] # 0番目は削除。これで_list[i]でそれぞれの列を抜き出すことができる。\n",
        "#del c_list[0]\n",
        "\n",
        "learning_list=[]\n",
        "for i in range(len(t_list)): #learning_list[0]は一番上の論文\n",
        "  learning_words=t_list[i]+a_list[i] #+c_list[i]\n",
        "  learning_list.append(learning_words)\n",
        "\n",
        "predict_lists=[]\n",
        "\"\"\" #alldatalearning\n",
        "for i,k in enumerate(learning_list):\n",
        "  Ta=data_change(k)\n",
        "  predict_num=learning_predicting(Ta[0],Ta[1],Ta[2],Ta[3]) #出力data_changeでデータをもらってlearning_predictingで予測結果をもらう。\n",
        "  predict_lists.append([p_list[i]])\n",
        "  #del predict_num[0]\n",
        "  for f in range(len(predict_num)):\n",
        "    if f!=0:\n",
        "      predict_lists[i].append(predict_num[f]*100)\n",
        "    #predict_lists[i].append(f[0]*100)\n",
        "\"\"\" \n",
        "#solodatalearning\n",
        "for i in range(len(learning_list)):\n",
        "  Ta=data_change(learning_list[i])\n",
        "  predict_num=learning_predicting(Ta[0],Ta[1],Ta[2],Ta[3]) #出力data_changeでデータをもらってlearning_predictingで予測結果をもらう。\n",
        "  predict_lists.append([p_list[i]])\n",
        "  #del predict_num[0]\n",
        "  for f in range(len(predict_num)):\n",
        "    if f!=0:\n",
        "      predict_lists[i].append(predict_num[f]*100)\n",
        "    #predict_lists[i].append(f[0]*100)\n",
        "\n",
        "  \n",
        "  \"\"\"\n",
        "  ranking_check=[]\n",
        "  for j in predict_num[0:3]:\n",
        "    ranking_check.append(j[1])\n",
        "\n",
        "  if int(p_list[i]) in ranking_check:\n",
        "    predict_lists.append([p_list[i],1])\n",
        "  else:\n",
        "    predict_lists.append([p_list[i],0])\n",
        "  \"\"\"\n",
        "with open('some.csv', 'w') as f:\n",
        "    writer = csv.writer(f, lineterminator='\\n') # 改行コード（\\n）を指定しておく\n",
        "    writer.writerows(predict_lists) # 2次元配列も書き込める"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}