{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine-STI2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hatahataDev/SDGsproject/blob/master/Machine_STI2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vbpcFfVEHe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive # グーグルドライブをマウントさせるコード。\n",
        "drive.mount('/content/drive')\n",
        "# dir=\"drive/My Drive/**/text\" # drive内に入れたtextのパスを書く。\n",
        "data_dir=\"drive/My Drive/**/STI2.csv\" # CSVファイルのパス\n",
        "#predict_dir=\"drive/My Drive/**/needpredict.csv\"# 予測データのパス"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgCPXcwURqXx",
        "colab_type": "code",
        "outputId": "850b5e69-f12e-493e-956c-4c77af47bacd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "\"\"\"\n",
        "入力用データ成型機\n",
        "以下　Goal4の公式文\n",
        "By 2030, ensure that all young people and the majority (both men and women) adults have literacy and basic computing skills.\n",
        "By 2030, education for sustainable development and sustainable lifestyles, human rights, gender equality, promotion of peace and non-violent culture, global citizenship, sustainable development of cultural diversity and culture Ensure that all learners have the knowledge and skills necessary to promote sustainable development through education in understanding their contributions.\n",
        "Establish and improve educational facilities that are child, disability and gender sensitive and provide a safe, non-violent, inclusive and effective learning environment for all.\n",
        "By 2020, developed countries such as vocational training, information and communication technology (ICT), technology, engineering and science programs for developing countries, especially least developed and small island developing countries, and African countries Significantly increase the number of higher education scholarships worldwide in other developing countries.\n",
        "\n",
        "\"\"\"\n",
        "# https://qiita.com/shimayu22/items/4fc6d01a4bf0eef909a0 参考記事\n",
        "# https://qiita.com/shimayu22/items/4fc6d01a4bf0eef909a0\n",
        "# 課題～ハイパーパラメータをいじる、あるいはデータの抜き出し、整形の制度を上げる。\n",
        "# 数値のベクトル化のところのアルゴリズムが少し不自然な気がするからそこの修正\n",
        "# ネット回線ないとコード動きません。\n",
        "# ネットのSDGs用語も辞書に足し合わせるべし←ネットに乗っている用語は重要度を上げるなどの改良の余地あり。\n",
        "# 大文字を小文字にする処理や記号を抜き取る処理を追加する。\n",
        "import os\n",
        "import csv\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('punkt') # nltkを使うためのソフト群\n",
        "nltk.download('tagsets')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "csv_file = open(data_dir,'r') # 学習データ\n",
        "p_n_list = input(\"予測したい文章を入力してください。\")\n",
        "# p_csv_file= open(predict_dir,\"r\") # 予測したいデータ\n",
        "\"\"\"\n",
        "csvファイルは左から目標番号、タイトル、アブスト、結論の順で入れること。\n",
        "\"\"\"\n",
        "p_list,t_list,a_list = [],[],[]\n",
        "# c_list= []\n",
        "\n",
        "delete= [\"a\",\"%\",\"of\",\"in\",\"on\",\"to\",\"for\",\"are\",\"at\",\"[\",\"]\"] # 関連性のない語を集めた配列\n",
        "\n",
        "# p_n_list=[]#予測したいデータのテキストバッグ\n",
        "\n",
        "# for row in csv.reader(p_csv_file):\n",
        "# p_n_list.append(row[1])\n",
        "# p_n_list.append(row[2])\n",
        "\n",
        "x, y = [], []\n",
        "count = 0\n",
        "\n",
        "for row in csv.reader(csv_file):\n",
        "  # 以下リストを作成\n",
        "    p_list.append(row[0]) # 取得したい列番号を指定（0始まり）#目標番号をすべて取得\n",
        "    t_list.append(row[1]) # タイトルをすべて取得\n",
        "    a_list.append(row[2]) # アブストをすべて取得\n",
        "    # c_list.append(row[3]) # 結論も入れる\n",
        "\n",
        "del p_list[0] # 0番目を削除\n",
        "del t_list[0] # 0番目は削除\n",
        "del a_list[0] # 0番目は削除。これで_list[i]でそれぞれの列を抜き出すことができる。\n",
        "# del c_list[0]\n",
        "\n",
        "# 以下３行　予測したいデータの形態素解析\n",
        "# p_n_list=p_n_list[0]+p_n_list[1] \n",
        "p_n_words = nltk.word_tokenize(p_n_list)\n",
        "p_n_wards=nltk.pos_tag(p_n_words)# [(\"単語\",\"品詞\")....]の２次元リストをここで生成\n",
        "\n",
        "\n",
        "p_result = [] # 名詞の数字配列を入れるp_resultを定義\n",
        "p_N_dic = [] # 名詞のstring型配列を定義\n",
        "# 以下名詞を抜き出しN_dicに入れ込み\n",
        "for word,part in p_n_wards: # wordに単語、partに品詞が入っている→このforはタイトル\n",
        "    if part==\"NN\" or part==\"NNP\" or part==\"VBP\": # 以下if文は名詞のみをN_dicというリストに格納\n",
        "      if not word.lower() in delete: # さらにdeleteリストに入っている意味のない名詞を削除\n",
        "        sword=word.lower()\n",
        "        p_N_dic.append(sword)\n",
        "\n",
        "Cdic=[] # 各ゴールの単語をすべて入れる。推薦単語用.Cdic[1]→1番目の単語すべてを返す\n",
        "\n",
        "for i in range(18): # それぞれのゴールの中身を内包した収納配列を生成\n",
        "  a=[]\n",
        "  Cdic.append(a) # [[],[],[]......18個]この中に単語をすべて入れていく\n",
        "\n",
        "dic={} # 辞書で定義\n",
        "for i,p in enumerate(p_list): # p_listのiをカウンタ変数,ｐを正解ラベル\n",
        "  j=i+1 # p_list,a_listなどは0番目が存在しないため無理やりjをカウンター変数に置き換え\n",
        "  if j>=len(p_list): # +1してるから最後の一つだけ存在しないjが入るためここで分岐処理　t_list[j_max]は存在しないためエラー\n",
        "    break\n",
        "  N_dic=[] # ここで名詞リストは初期化\n",
        "  # print(t_list[i])\n",
        "  t_string = t_list[j]\n",
        "  t_words = nltk.word_tokenize(t_string)\n",
        "  t_wards=nltk.pos_tag(t_words)# [(\"単語\",\"品詞\")....]の２次元リストをここで生成\n",
        "  # print(t_wards)\n",
        "  a_string = a_list[j]\n",
        "  a_words = nltk.word_tokenize(a_string)\n",
        "  a_wards=nltk.pos_tag(a_words)\n",
        "  \"\"\"\n",
        "  c_string = c_list[j]\n",
        "  c_words = nltk.word_tokenize(c_string)\n",
        "  c_wards=nltk.pos_tag(c_words)\n",
        "  \"\"\"\n",
        "  for word,part in t_wards: # wordに単語、partに品詞が入っている→このforはタイトル\n",
        "    if part==\"NN\" or part==\"NNP\" or part==\"VBP\" or part==\"NNS\" or part==\"NNPS\": # 以下if文は名詞のみをN_dicというリストに格納\n",
        "      if not word.lower() in delete: #さらにdeleteリストに入っている意味のない名詞を削除\n",
        "        sword=word.lower() # 小文字に変換\n",
        "        N_dic.append(sword)\n",
        "        Cdic[int(p)].extend([sword]) # swordはstr型なので無理やりリストに変換 extendはlist型しか入らない\n",
        "  for word,part in a_wards: # wordに単語、partに品詞が入っている→このforはアブスト\n",
        "    if part==\"NN\" or part==\"NNP\" or part==\"VBP\" or part==\"NNS\" or part==\"NNPS\": # 以下if文は名詞のみをN_dicというリストに格納\n",
        "      if not word.lower() in delete:\n",
        "        sword=word.lower()\n",
        "        N_dic.append(sword)\n",
        "        Cdic[int(p)].extend([sword]) # pはstrなのでキャスト\n",
        "  \"\"\"\n",
        "  for word,part in c_wards: #wordに単語、partに品詞が入っている→このforはアブスト\n",
        "    if part==\"NN\": #以下if文は名詞のみをN_dicというリストに格納\n",
        "      N_dic.append(word)\n",
        "    if part==\"NNP\":\n",
        "      N_dic.append(word)\n",
        "  \"\"\"\n",
        " # print(N_dic)# すべてN_dicにまとめてしまう。\n",
        "\n",
        "  result=[]\n",
        "  for word in N_dic:\n",
        "    word = word.strip()  \n",
        "    if word == \"\": continue\n",
        "    if not word in dic: # 未登録の場合\n",
        "      dic[word] = count  # count の数字で辞書に登録　補足：辞書型はwordとkeyの二つの情報を入れ込むことができる.これでNNに入れれるようにしていく\n",
        "      num = count\n",
        "      count +=1\n",
        "    # print(num,word)  # 数字と単語を表示\n",
        "    else:\n",
        "      num=dic[word] # 数字を辞書で調べる\n",
        "    result.append(num)  # リストに数字を追加 ここで名詞の塊を数値化している\n",
        "  x.append(result)  # リストを配列 ｘ に追加 x = [ [ 10, 11, 12, 3, 8, 13,  … 38 ] , [ 39, 40, 12, 16, 19, … ,56 ] , ….  ,  [ 1200, 1504, 3, 15, … 3300 ] ] \n",
        "  p_p=int(p)  # pは文字型で入っているのでキャスト\n",
        "  y.append(p_p)  # 正解ラベルを配列 y に追加 y = [ 0, 0, 0, 0, … ,8, 8, 8, 8 ] \n",
        "\n",
        "# 以下のfor 予測したいデータの名詞配列の数値化 or 辞書の生成\n",
        "for word in p_N_dic:\n",
        "    word = word.strip()  \n",
        "    if word == \"\": continue\n",
        "    if not word in dic: # 未登録の場合\n",
        "      dic[word] = count  # count の数字で辞書に登録　補足：辞書型はwordとkeyの二つの情報を入れ込むことができる.これでNNに入れれるようにしていく\n",
        "      num = count\n",
        "      count +=1\n",
        "    # print(num,word)  # 数字と単語を表示\n",
        "    else:\n",
        "      num=dic[word] # 数字を辞書で調べる\n",
        "    p_result.append(num)  # リストに数字を追加 ここで名詞の塊を数値化している\n",
        "    \n",
        "#print(x)\n",
        "#print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "予測したい文章を入力してください。By 2030, ensure that all young people and the majority (both men and women) adults have literacy and basic computing skills. By 2030, education for sustainable development and sustainable lifestyles, human rights, gender equality, promotion of peace and non-violent culture, global citizenship, sustainable development of cultural diversity and culture Ensure that all learners have the knowledge and skills necessary to promote sustainable development through education in understanding their contributions. Establish and improve educational facilities that are child, disability and gender sensitive and provide a safe, non-violent, inclusive and effective learning environment for all. By 2020, developed countries such as vocational training, information and communication technology (ICT), technology, engineering and science programs for developing countries, especially least developed and small island developing countries, and African countries Significantly increase the number of higher education scholarships worldwide in other developing countries.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfMeRUxNEICt",
        "colab_type": "code",
        "outputId": "b7f0053b-a36e-44ec-9543-7a58a687eb82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#学習機\n",
        "import numpy as np\n",
        "import keras\n",
        "import glob\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split ### 追加\n",
        "from collections import Counter #カウントするメソッドを使うためのモジュール\n",
        "\n",
        "max_words = len(dic)\n",
        "batch_size = 150\n",
        "epochs = 30\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=111)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        " \n",
        "num_classes = np.max(y_train) +1\n",
        "print(num_classes, 'classes')\n",
        " \n",
        "print('Vectorizing sequence data...')\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        " \n",
        "print('Convert class vector to binary class matrix '\n",
        "'(for use with categorical_crossentropy)')\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "\n",
        "# modelのインスタンス生成\n",
        "print('Building model...')\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))#２層ニューラルネットワーク\n",
        "\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "optimizer='adam',\n",
        "metrics=['accuracy']) \n",
        " \n",
        "history = model.fit(x_train, y_train,\n",
        "batch_size=batch_size,\n",
        "epochs=epochs,\n",
        "verbose=1,\n",
        "validation_split=0.1)\n",
        "score = model.evaluate(x_test, y_test,\n",
        "batch_size=batch_size, verbose=1)\n",
        "\n",
        "pre=model.predict( x_test, batch_size=batch_size, verbose=1) #それぞれの正解ラベルに対する確率を返す\n",
        "\n",
        "#予測データのバイナリー化\n",
        "def binary(result):\n",
        "  arr=np.zeros(len(dic)) #辞書数分のバイナリー配列を生成。すべて０で最初は生成\n",
        "  for f in result: #予測したい文脈の名詞の番号のところに１を立てる。\n",
        "    arr[f]=1\n",
        "\n",
        "  arr=np.array(arr) #numpy配列に変換\n",
        "  arr.reshape(1,len(dic))\n",
        "  arrd=np.stack([arr, arr]) #無理やり配列を拡張。model.predictの引数に拡張しないと入れられないため。(2,len(dic))の配列にする\n",
        "  return arrd\n",
        "#print(arrd.shape)\n",
        "bin_pre=binary(p_result)\n",
        "pre1=model.predict( bin_pre, batch_size=batch_size, verbose=1)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "#print(pre1)\n",
        "\n",
        "#出力関数\n",
        "def result(pre):\n",
        "  result= []\n",
        "  for i,c in enumerate(pre[0]): #２次元配列を生成\n",
        "    hum=[c,i]\n",
        "    result.append(hum)\n",
        "  result2=sorted(result,reverse=True) #大きい順にソート\n",
        "\n",
        "  j=0\n",
        "  for k,l in enumerate(result2):\n",
        "    if l[1]==0:\n",
        "      j=1\n",
        "      continue\n",
        "    if j==1:\n",
        "      print(str(k)+\"番目に関連が高いのは\"+str(l[1])+\"の目標で,関連度は\"+str(l[0]*100)+\"％です。\")\n",
        "    else:\n",
        "      print(str(k+1)+\"番目に関連が高いのは\"+str(l[1])+\"の目標で,関連度は\"+str(l[0]*100)+\"％です\")\n",
        "  return result2\n",
        "current_result=result(pre1)\n",
        "\n",
        "#推薦単語アルゴリズム\n",
        "def renumbering(new_word,p_result): #推薦単語をすべて包括した場合の予測 任意の単語を追加して単語を数値化\n",
        "  jaka=[]\n",
        "  for word in new_word:\n",
        "      word = word.strip()  \n",
        "      if word == \"\": continue\n",
        "      if not word in dic: # 未登録の場合\n",
        "        dic[word] = count  # count の数字で辞書に登録　補足：辞書型はwordとkeyの二つの情報を入れ込むことができる.これでNNに入れれるようにしていく\n",
        "        num = count\n",
        "        count +=1\n",
        "      #print(num,word)  # 数字と単語を表示\n",
        "      else:\n",
        "        num=dic[word] # 数字を辞書で調べる\n",
        "        jaka.append(num)  # リストに数字を追加 ここで名詞の塊を数値化している\n",
        "  return p_result+jaka\n",
        "\n",
        "  \n",
        "G=Counter(Cdic[current_result[0][1]]).most_common() #それぞれのゴール単語を包括した辞書を予測論文の１番関連度が高いゴールの単語を多い順に返してくれる。ここで型をリストに変えてくれる\n",
        "print(G[0])\n",
        "\n",
        "new_words=[] #表示させる推薦単語\n",
        "for i,k in enumerate(G):\n",
        "  if i==10: #上位の単語をいくつ調べるかの設定。\n",
        "    break\n",
        "  if k[0] in p_N_dic: #その論文に推薦単語がある場合処理をスキップ\n",
        "    continue\n",
        "  print(\"他に包括したほうが良い単語は\"+k[0]+\"です。\\n\") \n",
        "  new_words.append(k[0])\n",
        "#G[0][0] [出現単語][出現回数]\n",
        "new_predict=renumbering(new_words,p_result)\n",
        "repredict=binary(new_predict) #2値化\n",
        "pre2=model.predict( repredict, batch_size=batch_size, verbose=1) #予測データ取得\n",
        "print(\"上記の単語を包括した場合の関連度は以下の様になります。\")\n",
        "result(pre2) #出力\n",
        "\n",
        "print(\"その他のゴールも包括するには以下のような単語を推薦します\")\n",
        "for i in range(18):\n",
        "  if i==0:\n",
        "    continue\n",
        "  if i==current_result[0][1]:\n",
        "    continue\n",
        "  G=Counter(Cdic[i]).most_common()\n",
        "  print(\"Goal\"+str(i)+\"⇩\")\n",
        "  for i,k in enumerate(G):\n",
        "    if i==10: #上位の単語をいくつ調べるかの設定。\n",
        "      break\n",
        "    if k[0] in p_N_dic: #その論文に推薦単語がある場合処理をスキップ\n",
        "      continue\n",
        "    print(\"他に包括したほうが良い単語は\"+k[0]+\"です。\\n\") \n",
        "\n",
        "### Plot accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history[\"acc\"]\n",
        "val_acc = history.history[\"val_acc\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        " \n",
        "plt.plot(epochs, acc, \"bo\", label = \"Training acc\" )\n",
        "plt.plot(epochs, val_acc, \"b\", label = \"Validation acc\")\n",
        "plt.title(\"Training and Validation accuracy\")\n",
        "plt.legend()\n",
        "plt.savefig(\"acc.png\")\n",
        "plt.close()\n",
        " \n",
        "### plot Confusion Matrix\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix\n",
        " \n",
        "def print_cmx(y_true, y_pred):\n",
        "  labels = sorted(list(set(y_true)))\n",
        "  cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
        " \n",
        "  df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
        " \n",
        "  plt.figure(figsize = (10,7))\n",
        "  sn.heatmap(df_cmx, annot=True, fmt=\"d\") ### ヒートマップの表示仕様\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  plt.xlabel(\"predict_classes\")\n",
        "  plt.ylabel(\"true_classes\")\n",
        "  plt.savefig(\"c_matrix.png\")\n",
        "  plt.close()\n",
        " \n",
        "predict_classes = model.predict_classes(x_test[1:10000,], batch_size=32) ### 予測したラベルを取得\n",
        "true_classes = np.argmax(y_test[1:10000],1) ### 実際のラベルを取得\n",
        "print(confusion_matrix(true_classes, predict_classes))\n",
        "print_cmx(true_classes, predict_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "268 train sequences\n",
            "68 test sequences\n",
            "18 classes\n",
            "Vectorizing sequence data...\n",
            "x_train shape: (268, 5071)\n",
            "x_test shape: (68, 5071)\n",
            "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
            "y_train shape: (268, 18)\n",
            "y_test shape: (68, 18)\n",
            "Building model...\n",
            "Train on 241 samples, validate on 27 samples\n",
            "Epoch 1/30\n",
            "241/241 [==============================] - 1s 4ms/step - loss: 2.9171 - acc: 0.0996 - val_loss: 2.7058 - val_acc: 0.2963\n",
            "Epoch 2/30\n",
            "241/241 [==============================] - 0s 582us/step - loss: 2.6515 - acc: 0.2614 - val_loss: 2.5311 - val_acc: 0.2963\n",
            "Epoch 3/30\n",
            "241/241 [==============================] - 0s 588us/step - loss: 2.3674 - acc: 0.4149 - val_loss: 2.3506 - val_acc: 0.2593\n",
            "Epoch 4/30\n",
            "241/241 [==============================] - 0s 577us/step - loss: 2.0592 - acc: 0.4813 - val_loss: 2.2196 - val_acc: 0.2222\n",
            "Epoch 5/30\n",
            "241/241 [==============================] - 0s 583us/step - loss: 1.8301 - acc: 0.4523 - val_loss: 2.1768 - val_acc: 0.2222\n",
            "Epoch 6/30\n",
            "241/241 [==============================] - 0s 586us/step - loss: 1.6462 - acc: 0.4647 - val_loss: 2.1674 - val_acc: 0.2593\n",
            "Epoch 7/30\n",
            "241/241 [==============================] - 0s 569us/step - loss: 1.4177 - acc: 0.5851 - val_loss: 2.1390 - val_acc: 0.3704\n",
            "Epoch 8/30\n",
            "241/241 [==============================] - 0s 586us/step - loss: 1.2128 - acc: 0.6515 - val_loss: 2.1152 - val_acc: 0.3704\n",
            "Epoch 9/30\n",
            "241/241 [==============================] - 0s 585us/step - loss: 1.0716 - acc: 0.7261 - val_loss: 2.1198 - val_acc: 0.3704\n",
            "Epoch 10/30\n",
            "241/241 [==============================] - 0s 559us/step - loss: 0.8851 - acc: 0.7801 - val_loss: 2.1315 - val_acc: 0.4074\n",
            "Epoch 11/30\n",
            "241/241 [==============================] - 0s 599us/step - loss: 0.7541 - acc: 0.8299 - val_loss: 2.1582 - val_acc: 0.3704\n",
            "Epoch 12/30\n",
            "241/241 [==============================] - 0s 592us/step - loss: 0.5759 - acc: 0.8880 - val_loss: 2.2018 - val_acc: 0.3704\n",
            "Epoch 13/30\n",
            "241/241 [==============================] - 0s 573us/step - loss: 0.4938 - acc: 0.9087 - val_loss: 2.2539 - val_acc: 0.4074\n",
            "Epoch 14/30\n",
            "241/241 [==============================] - 0s 553us/step - loss: 0.4039 - acc: 0.9295 - val_loss: 2.2968 - val_acc: 0.3704\n",
            "Epoch 15/30\n",
            "241/241 [==============================] - 0s 568us/step - loss: 0.3527 - acc: 0.9336 - val_loss: 2.3245 - val_acc: 0.3704\n",
            "Epoch 16/30\n",
            "241/241 [==============================] - 0s 585us/step - loss: 0.2879 - acc: 0.9544 - val_loss: 2.3425 - val_acc: 0.3704\n",
            "Epoch 17/30\n",
            "241/241 [==============================] - 0s 574us/step - loss: 0.2398 - acc: 0.9544 - val_loss: 2.3598 - val_acc: 0.3704\n",
            "Epoch 18/30\n",
            "241/241 [==============================] - 0s 560us/step - loss: 0.1944 - acc: 0.9668 - val_loss: 2.3807 - val_acc: 0.4074\n",
            "Epoch 19/30\n",
            "241/241 [==============================] - 0s 577us/step - loss: 0.1474 - acc: 0.9876 - val_loss: 2.4109 - val_acc: 0.4074\n",
            "Epoch 20/30\n",
            "241/241 [==============================] - 0s 590us/step - loss: 0.1349 - acc: 0.9834 - val_loss: 2.4455 - val_acc: 0.4444\n",
            "Epoch 21/30\n",
            "241/241 [==============================] - 0s 549us/step - loss: 0.1044 - acc: 0.9917 - val_loss: 2.4863 - val_acc: 0.4444\n",
            "Epoch 22/30\n",
            "241/241 [==============================] - 0s 571us/step - loss: 0.0828 - acc: 0.9917 - val_loss: 2.5209 - val_acc: 0.4815\n",
            "Epoch 23/30\n",
            "241/241 [==============================] - 0s 566us/step - loss: 0.0772 - acc: 0.9917 - val_loss: 2.5619 - val_acc: 0.4815\n",
            "Epoch 24/30\n",
            "241/241 [==============================] - 0s 585us/step - loss: 0.0654 - acc: 1.0000 - val_loss: 2.6021 - val_acc: 0.4815\n",
            "Epoch 25/30\n",
            "241/241 [==============================] - 0s 554us/step - loss: 0.0514 - acc: 0.9959 - val_loss: 2.6443 - val_acc: 0.4815\n",
            "Epoch 26/30\n",
            "241/241 [==============================] - 0s 585us/step - loss: 0.0368 - acc: 1.0000 - val_loss: 2.6888 - val_acc: 0.4444\n",
            "Epoch 27/30\n",
            "241/241 [==============================] - 0s 629us/step - loss: 0.0325 - acc: 1.0000 - val_loss: 2.7361 - val_acc: 0.4444\n",
            "Epoch 28/30\n",
            "241/241 [==============================] - 0s 561us/step - loss: 0.0283 - acc: 1.0000 - val_loss: 2.7820 - val_acc: 0.4444\n",
            "Epoch 29/30\n",
            "241/241 [==============================] - 0s 580us/step - loss: 0.0255 - acc: 1.0000 - val_loss: 2.8241 - val_acc: 0.4444\n",
            "Epoch 30/30\n",
            "241/241 [==============================] - 0s 576us/step - loss: 0.0271 - acc: 1.0000 - val_loss: 2.8612 - val_acc: 0.4074\n",
            "68/68 [==============================] - 0s 145us/step\n",
            "68/68 [==============================] - 0s 2ms/step\n",
            "2/2 [==============================] - 0s 2ms/step\n",
            "Test score: 1.8391306400299072\n",
            "Test accuracy: 0.4852941036224365\n",
            "1番目に関連が高いのは4の目標で,関連度は53.17385792732239％です\n",
            "2番目に関連が高いのは3の目標で,関連度は12.29073852300644％です\n",
            "3番目に関連が高いのは14の目標で,関連度は10.65143495798111％です\n",
            "4番目に関連が高いのは9の目標で,関連度は6.9799162447452545％です\n",
            "5番目に関連が高いのは11の目標で,関連度は4.459061101078987％です\n",
            "6番目に関連が高いのは12の目標で,関連度は3.8428422063589096％です\n",
            "7番目に関連が高いのは6の目標で,関連度は2.3508258163928986％です\n",
            "8番目に関連が高いのは7の目標で,関連度は2.155062183737755％です\n",
            "9番目に関連が高いのは2の目標で,関連度は1.4488247223198414％です\n",
            "10番目に関連が高いのは15の目標で,関連度は0.9029211476445198％です\n",
            "11番目に関連が高いのは13の目標で,関連度は0.7372288964688778％です\n",
            "12番目に関連が高いのは17の目標で,関連度は0.6539114750921726％です\n",
            "13番目に関連が高いのは8の目標で,関連度は0.24235127493739128％です\n",
            "14番目に関連が高いのは1の目標で,関連度は0.030834198696538806％です\n",
            "15番目に関連が高いのは16の目標で,関連度は0.024303322425112128％です\n",
            "16番目に関連が高いのは5の目標で,関連度は0.023860226792749017％です\n",
            "17番目に関連が高いのは10の目標で,関連度は0.012674910249188542％です。\n",
            "('students', 33)\n",
            "他に包括したほうが良い単語はstudentsです。\n",
            "\n",
            "他に包括したほうが良い単語はenglishです。\n",
            "\n",
            "他に包括したほうが良い単語はmetacognitionです。\n",
            "\n",
            "他に包括したほうが良い単語はcollegeです。\n",
            "\n",
            "他に包括したほうが良い単語はactivityです。\n",
            "\n",
            "他に包括したほうが良い単語はstudyです。\n",
            "\n",
            "2/2 [==============================] - 0s 1ms/step\n",
            "上記の単語を包括した場合の関連度は以下の様になります。\n",
            "1番目に関連が高いのは4の目標で,関連度は94.53668594360352％です\n",
            "2番目に関連が高いのは3の目標で,関連度は2.3817410692572594％です\n",
            "3番目に関連が高いのは14の目標で,関連度は1.036152709275484％です\n",
            "4番目に関連が高いのは9の目標で,関連度は0.8131691254675388％です\n",
            "5番目に関連が高いのは11の目標で,関連度は0.4451795946806669％です\n",
            "6番目に関連が高いのは12の目標で,関連度は0.301741692237556％です\n",
            "7番目に関連が高いのは6の目標で,関連度は0.17818192718550563％です\n",
            "8番目に関連が高いのは7の目標で,関連度は0.10121979285031557％です\n",
            "9番目に関連が高いのは2の目標で,関連度は0.08227043435908854％です\n",
            "10番目に関連が高いのは13の目標で,関連度は0.042140859295614064％です\n",
            "11番目に関連が高いのは15の目標で,関連度は0.03759634855668992％です\n",
            "12番目に関連が高いのは17の目標で,関連度は0.030331380548886955％です\n",
            "13番目に関連が高いのは8の目標で,関連度は0.011375905887689441％です\n",
            "14番目に関連が高いのは1の目標で,関連度は0.0006874610789964208％です\n",
            "15番目に関連が高いのは5の目標で,関連度は0.0004739906671602512％です\n",
            "16番目に関連が高いのは16の目標で,関連度は0.0004503159743762808％です\n",
            "17番目に関連が高いのは10の目標で,関連度は0.0001889651002784376％です。\n",
            "その他のゴールも包括するには以下のような単語を推薦します\n",
            "Goal1⇩\n",
            "他に包括したほうが良い単語はonionです。\n",
            "\n",
            "他に包括したほうが良い単語はcutです。\n",
            "\n",
            "他に包括したほうが良い単語はcultivationです。\n",
            "\n",
            "他に包括したほうが良い単語はsoilです。\n",
            "\n",
            "他に包括したほうが良い単語は’です。\n",
            "\n",
            "他に包括したほうが良い単語はhinaiです。\n",
            "\n",
            "他に包括したほうが良い単語はimprovementです。\n",
            "\n",
            "他に包括したほうが良い単語はmaterialです。\n",
            "\n",
            "他に包括したほうが良い単語はproductivityです。\n",
            "\n",
            "他に包括したほうが良い単語はyieldsです。\n",
            "\n",
            "Goal2⇩\n",
            "他に包括したほうが良い単語はsystemです。\n",
            "\n",
            "他に包括したほうが良い単語はnematodesです。\n",
            "\n",
            "他に包括したほうが良い単語はcompostです。\n",
            "\n",
            "他に包括したほうが良い単語はsludgeです。\n",
            "\n",
            "他に包括したほうが良い単語はreactorです。\n",
            "\n",
            "他に包括したほうが良い単語はwaterです。\n",
            "\n",
            "他に包括したほうが良い単語はsewageです。\n",
            "\n",
            "他に包括したほうが良い単語はstudyです。\n",
            "\n",
            "他に包括したほうが良い単語はbacillusです。\n",
            "\n",
            "他に包括したほうが良い単語はcultivationです。\n",
            "\n",
            "Goal3⇩\n",
            "他に包括したほうが良い単語はcellです。\n",
            "\n",
            "他に包括したほうが良い単語はearthwormです。\n",
            "\n",
            "他に包括したほうが良い単語はmethodです。\n",
            "\n",
            "他に包括したほうが良い単語はsystemです。\n",
            "\n",
            "他に包括したほうが良い単語はstudyです。\n",
            "\n",
            "他に包括したほうが良い単語はcellsです。\n",
            "\n",
            "他に包括したほうが良い単語はearthwormsです。\n",
            "\n",
            "Goal5⇩\n",
            "Goal6⇩\n",
            "他に包括したほうが良い単語はreactorです。\n",
            "\n",
            "他に包括したほうが良い単語はwastewaterです。\n",
            "\n",
            "他に包括したほうが良い単語はtreatmentです。\n",
            "\n",
            "他に包括したほうが良い単語はdhsです。\n",
            "\n",
            "他に包括したほうが良い単語はprocessです。\n",
            "\n",
            "他に包括したほうが良い単語はremovalです。\n",
            "\n",
            "他に包括したほうが良い単語はsystemです。\n",
            "\n",
            "他に包括したほうが良い単語はsludgeです。\n",
            "\n",
            "他に包括したほうが良い単語はrateです。\n",
            "\n",
            "他に包括したほうが良い単語はstudyです。\n",
            "\n",
            "Goal7⇩\n",
            "他に包括したほうが良い単語はmethodです。\n",
            "\n",
            "他に包括したほうが良い単語はenergyです。\n",
            "\n",
            "他に包括したほうが良い単語はstudyです。\n",
            "\n",
            "他に包括したほうが良い単語はreeseiです。\n",
            "\n",
            "他に包括したほうが良い単語はanalysisです。\n",
            "\n",
            "他に包括したほうが良い単語はsurfaceです。\n",
            "\n",
            "他に包括したほうが良い単語はcellulaseです。\n",
            "\n",
            "他に包括したほうが良い単語はstructureです。\n",
            "\n",
            "他に包括したほうが良い単語はcellです。\n",
            "\n",
            "Goal8⇩\n",
            "他に包括したほうが良い単語はlaserです。\n",
            "\n",
            "他に包括したほうが良い単語はablationです。\n",
            "\n",
            "他に包括したほうが良い単語はsolutionです。\n",
            "\n",
            "他に包括したほうが良い単語はnmです。\n",
            "\n",
            "他に包括したほうが良い単語はformationです。\n",
            "\n",
            "他に包括したほうが良い単語はnanoparticlesです。\n",
            "\n",
            "他に包括したほうが良い単語はabsorptionです。\n",
            "\n",
            "他に包括したほうが良い単語はspectraです。\n",
            "\n",
            "他に包括したほうが良い単語はcbepaです。\n",
            "\n",
            "他に包括したほうが良い単語はsurfactantsです。\n",
            "\n",
            "Goal9⇩\n",
            "他に包括したほうが良い単語はstudyです。\n",
            "\n",
            "他に包括したほうが良い単語はmethodです。\n",
            "\n",
            "他に包括したほうが良い単語はwaterです。\n",
            "\n",
            "他に包括したほうが良い単語はsurfaceです。\n",
            "\n",
            "他に包括したほうが良い単語はsystemです。\n",
            "\n",
            "他に包括したほうが良い単語はgrowthです。\n",
            "\n",
            "他に包括したほうが良い単語はresearchです。\n",
            "\n",
            "他に包括したほうが良い単語はtimeです。\n",
            "\n",
            "Goal10⇩\n",
            "Goal11⇩\n",
            "他に包括したほうが良い単語はcityです。\n",
            "\n",
            "他に包括したほうが良い単語はareaです。\n",
            "\n",
            "他に包括したほうが良い単語はresearchです。\n",
            "\n",
            "他に包括したほうが良い単語はstudyです。\n",
            "\n",
            "他に包括したほうが良い単語はplanningです。\n",
            "\n",
            "他に包括したほうが良い単語はmethodです。\n",
            "\n",
            "他に包括したほうが良い単語はwaterです。\n",
            "\n",
            "他に包括したほうが良い単語はpermeabilityです。\n",
            "\n",
            "他に包括したほうが良い単語はriverです。\n",
            "\n",
            "他に包括したほうが良い単語はflowです。\n",
            "\n",
            "Goal12⇩\n",
            "他に包括したほうが良い単語はashです。\n",
            "\n",
            "他に包括したほうが良い単語はcoalです。\n",
            "\n",
            "他に包括したほうが良い単語はradiationです。\n",
            "\n",
            "他に包括したほうが良い単語はmaterialsです。\n",
            "\n",
            "他に包括したほうが良い単語はstudyです。\n",
            "\n",
            "他に包括したほうが良い単語はlayerです。\n",
            "\n",
            "他に包括したほうが良い単語はbiofilmです。\n",
            "\n",
            "他に包括したほうが良い単語はcorrosionです。\n",
            "\n",
            "他に包括したほうが良い単語はgelです。\n",
            "\n",
            "Goal13⇩\n",
            "他に包括したほうが良い単語はlakeです。\n",
            "\n",
            "他に包括したほうが良い単語はimageです。\n",
            "\n",
            "他に包括したほうが良い単語はprocessingです。\n",
            "\n",
            "他に包括したほうが良い単語はrainfallです。\n",
            "\n",
            "他に包括したほうが良い単語はlayerです。\n",
            "\n",
            "他に包括したほうが良い単語はwaterです。\n",
            "\n",
            "他に包括したほうが良い単語はshinjiです。\n",
            "\n",
            "他に包括したほうが良い単語はresearchです。\n",
            "\n",
            "他に包括したほうが良い単語はvalueです。\n",
            "\n",
            "他に包括したほうが良い単語はsurfaceです。\n",
            "\n",
            "Goal14⇩\n",
            "他に包括したほうが良い単語はwaterです。\n",
            "\n",
            "他に包括したほうが良い単語はsystemです。\n",
            "\n",
            "他に包括したほうが良い単語はshapeです。\n",
            "\n",
            "他に包括したほうが良い単語はcopperです。\n",
            "\n",
            "他に包括したほうが良い単語はexchangeです。\n",
            "\n",
            "他に包括したほうが良い単語はstudyです。\n",
            "\n",
            "他に包括したほうが良い単語はmossです。\n",
            "\n",
            "他に包括したほうが良い単語はoptimizationです。\n",
            "\n",
            "他に包括したほうが良い単語はmicroalgaeです。\n",
            "\n",
            "他に包括したほうが良い単語はfreshwaterです。\n",
            "\n",
            "Goal15⇩\n",
            "他に包括したほうが良い単語はsurfaceです。\n",
            "\n",
            "他に包括したほうが良い単語はbfです。\n",
            "\n",
            "他に包括したほうが良い単語はexplantsです。\n",
            "\n",
            "他に包括したほうが良い単語はmembraneです。\n",
            "\n",
            "他に包括したほうが良い単語はplantです。\n",
            "\n",
            "他に包括したほうが良い単語はstudyです。\n",
            "\n",
            "他に包括したほうが良い単語はbacteriaです。\n",
            "\n",
            "他に包括したほうが良い単語はplantsです。\n",
            "\n",
            "他に包括したほうが良い単語はgrowthです。\n",
            "\n",
            "他に包括したほうが良い単語はdnaです。\n",
            "\n",
            "Goal16⇩\n",
            "Goal17⇩\n",
            "他に包括したほうが良い単語はplasmaです。\n",
            "\n",
            "他に包括したほうが良い単語はohです。\n",
            "\n",
            "他に包括したほうが良い単語はpressureです。\n",
            "\n",
            "他に包括したほうが良い単語はaquacultureです。\n",
            "\n",
            "他に包括したほうが良い単語はfoodです。\n",
            "\n",
            "他に包括したほうが良い単語はproductionです。\n",
            "\n",
            "他に包括したほうが良い単語はreductionです。\n",
            "\n",
            "他に包括したほうが良い単語はgenerationです。\n",
            "\n",
            "他に包括したほうが良い単語はliquidです。\n",
            "\n",
            "他に包括したほうが良い単語はradicalsです。\n",
            "\n",
            "[[0 1 0 0 0 0 0 0 0 3 0]\n",
            " [0 3 0 0 0 6 0 0 0 0 0]\n",
            " [0 2 3 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 6 0 0 0 0 0 0 0]\n",
            " [0 0 0 3 9 6 0 0 0 1 0]\n",
            " [0 0 0 0 1 9 4 0 0 0 0]\n",
            " [0 0 0 0 0 1 2 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 1 0 1 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTtFZu1H1R3d",
        "colab_type": "code",
        "outputId": "9727944f-d818-4826-edfe-6b12c2603b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "result2[1][1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-21fb17da5c9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'result2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UawtOacbMvmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "入力用データ成型機\n",
        "\"\"\"\n",
        "#https://qiita.com/shimayu22/items/4fc6d01a4bf0eef909a0 参考記事\n",
        "#https://qiita.com/shimayu22/items/4fc6d01a4bf0eef909a0\n",
        "#課題～ハイパーパラメータをいじる、あるいはデータの抜き出し、整形の制度を上げる。\n",
        "#数値のベクトル化のところのアルゴリズムが少し不自然な気がするからそこの修正\n",
        "#ネット回線ないとコード動きません。\n",
        "#ネットのSDGs用語も辞書に足し合わせるべし←ネットに乗っている用語は重要度を上げるなどの改良の余地あり。\n",
        "#大文字を小文字にする処理や記号を抜き取る処理を追加する。\n",
        "import os\n",
        "import csv\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('punkt') #nltkを使うためのソフト群\n",
        "nltk.download('tagsets')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "csv_file = open(data_dir,'r') #学習データ\n",
        "p_n_list = input(\"予測したい文章を入力してください。\")\n",
        "#p_csv_file= open(predict_dir,\"r\") #予測したいデータ\n",
        "\"\"\"\n",
        "csvファイルは左から目標番号、タイトル、アブスト、結論の順で入れること。\n",
        "\"\"\"\n",
        "p_list,t_list,a_list = [],[],[]\n",
        "#c_list= []\n",
        "\n",
        "delete= [\"a\",\"%\",\"of\",\"in\",\"on\",\"to\",\"for\",\"are\",\"at\",\"[\",\"]\"] #関連性のない語を集めた配列\n",
        "\n",
        "#p_n_list=[]#予測したいデータのテキストバッグ\n",
        "\n",
        "#for row in csv.reader(p_csv_file):\n",
        "#  p_n_list.append(row[1])\n",
        "#  p_n_list.append(row[2])\n",
        "\n",
        "x, y = [], []\n",
        "count = 0\n",
        "\n",
        "for row in csv.reader(csv_file):\n",
        "  #以下リストを作成\n",
        "    p_list.append(row[0]) #取得したい列番号を指定（0始まり）#目標番号をすべて取得\n",
        "    t_list.append(row[1]) #タイトルをすべて取得\n",
        "    a_list.append(row[2]) #アブストをすべて取得\n",
        "    #c_list.append(row[3]) #結論も入れる\n",
        "\n",
        "del p_list[0] #0番目を削除\n",
        "del t_list[0] #0番目は削除\n",
        "del a_list[0] #0番目は削除。これで_list[i]でそれぞれの列を抜き出すことができる。\n",
        "#del c_list[0]\n",
        "\n",
        "#以下３行　予測したいデータの形態素解析\n",
        "#p_n_list=p_n_list[0]+p_n_list[1] \n",
        "p_n_words = nltk.word_tokenize(p_n_list)\n",
        "p_n_wards=nltk.pos_tag(p_n_words)#[(\"単語\",\"品詞\")....]の２次元リストをここで生成\n",
        "\n",
        "\n",
        "p_result = [] #名詞の数字配列を入れるp_resultを定義\n",
        "p_N_dic = [] #名詞のstring型配列を定義\n",
        "#以下名詞を抜き出しN_dicに入れ込み\n",
        "for word,part in p_n_wards: #wordに単語、partに品詞が入っている→このforはタイトル\n",
        "    if part==\"NN\" or part==\"NNP\" or part==\"VBP\": #以下if文は名詞のみをN_dicというリストに格納\n",
        "      if not word.lower() in delete: #さらにdeleteリストに入っている意味のない名詞を削除\n",
        "        sword=word.lower()\n",
        "        p_N_dic.append(sword)\n",
        "\n",
        "Cdic=[] #各ゴールの単語をすべて入れる。推薦単語用.Cdic[1]→1番目の単語すべてを返す\n",
        "\n",
        "for i in range(18): #それぞれのゴールの中身を内包した収納配列を生成\n",
        "  a=[]\n",
        "  Cdic.append(a) #[[],[],[]......18個]この中に単語をすべて入れていく\n",
        "\n",
        "dic={} #辞書で定義\n",
        "for i,p in enumerate(p_list): #p_listのiをカウンタ変数,ｐを正解ラベル\n",
        "  j=i+1 #p_list,a_listなどは0番目が存在しないため無理やりjをカウンター変数に置き換え\n",
        "  if j>=len(p_list): #+1してるから最後の一つだけ存在しないjが入るためここで分岐処理　t_list[j_max]は存在しないためエラー\n",
        "    break\n",
        "  N_dic=[] #ここで名詞リストは初期化\n",
        "  #print(t_list[i])\n",
        "  t_string = t_list[j]\n",
        "  t_words = nltk.word_tokenize(t_string)\n",
        "  t_wards=nltk.pos_tag(t_words)#[(\"単語\",\"品詞\")....]の２次元リストをここで生成\n",
        "  #print(t_wards)\n",
        "  a_string = a_list[j]\n",
        "  a_words = nltk.word_tokenize(a_string)\n",
        "  a_wards=nltk.pos_tag(a_words)\n",
        "  \"\"\"\n",
        "  c_string = c_list[j]\n",
        "  c_words = nltk.word_tokenize(c_string)\n",
        "  c_wards=nltk.pos_tag(c_words)\n",
        "  \"\"\"\n",
        "  for word,part in t_wards: #wordに単語、partに品詞が入っている→このforはタイトル\n",
        "    if part==\"NN\" or part==\"NNP\" or part==\"VBP\" or part==\"NNS\" or part==\"NNPS\": #以下if文は名詞のみをN_dicというリストに格納\n",
        "      if not word.lower() in delete: #さらにdeleteリストに入っている意味のない名詞を削除\n",
        "        sword=word.lower() #小文字に変換\n",
        "        N_dic.append(sword)\n",
        "        Cdic[int(p)].extend([sword]) #swordはstr型なので無理やりリストに変換 extendはlist型しか入らない\n",
        "  for word,part in a_wards: #wordに単語、partに品詞が入っている→このforはアブスト\n",
        "    if part==\"NN\" or part==\"NNP\" or part==\"VBP\" or part==\"NNS\" or part==\"NNPS\": #以下if文は名詞のみをN_dicというリストに格納\n",
        "      if not word.lower() in delete:\n",
        "        sword=word.lower()\n",
        "        N_dic.append(sword)\n",
        "        Cdic[int(p)].extend([sword]) #pはstrなのでキャスト\n",
        "  \"\"\"\n",
        "  for word,part in c_wards: #wordに単語、partに品詞が入っている→このforはアブスト\n",
        "    if part==\"NN\": #以下if文は名詞のみをN_dicというリストに格納\n",
        "      N_dic.append(word)\n",
        "    if part==\"NNP\":\n",
        "      N_dic.append(word)\n",
        "  \"\"\"\n",
        "  print(N_dic)#すべてN_dicにまとめてしまう。\n",
        "\n",
        "  result=[]\n",
        "  for word in N_dic:\n",
        "    word = word.strip()  \n",
        "    if word == \"\": continue\n",
        "    if not word in dic: # 未登録の場合\n",
        "      dic[word] = count  # count の数字で辞書に登録　補足：辞書型はwordとkeyの二つの情報を入れ込むことができる.これでNNに入れれるようにしていく\n",
        "      num = count\n",
        "      count +=1\n",
        "    #print(num,word)  # 数字と単語を表示\n",
        "    else:\n",
        "      num=dic[word] # 数字を辞書で調べる\n",
        "    result.append(num)  # リストに数字を追加 ここで名詞の塊を数値化している\n",
        "  x.append(result)  # リストを配列 ｘ に追加 x = [ [ 10, 11, 12, 3, 8, 13,  … 38 ] , [ 39, 40, 12, 16, 19, … ,56 ] , ….  ,  [ 1200, 1504, 3, 15, … 3300 ] ] \n",
        "  p_p=int(p)  #pは文字型で入っているのでキャスト\n",
        "  y.append(p_p)  # 正解ラベルを配列 y に追加 y = [ 0, 0, 0, 0, … ,8, 8, 8, 8 ] \n",
        "\n",
        "#以下のfor 予測したいデータの名詞配列の数値化 or 辞書の生成\n",
        "for word in p_N_dic:\n",
        "    word = word.strip()  \n",
        "    if word == \"\": continue\n",
        "    if not word in dic: # 未登録の場合\n",
        "      dic[word] = count  # count の数字で辞書に登録　補足：辞書型はwordとkeyの二つの情報を入れ込むことができる.これでNNに入れれるようにしていく\n",
        "      num = count\n",
        "      count +=1\n",
        "    #print(num,word)  # 数字と単語を表示\n",
        "    else:\n",
        "      num=dic[word] # 数字を辞書で調べる\n",
        "    p_result.append(num)  # リストに数字を追加 ここで名詞の塊を数値化している\n",
        "    \n",
        "print(x)\n",
        "print(y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQM96tuu7P5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#バイナリー化の証明\n",
        "#x2a=x_test[2]\n",
        "print(x2a)\n",
        "print(x_test[2])\n",
        "\n",
        "arr2=np.zeros(5242)\n",
        "\n",
        "for f in x2a:\n",
        "  arr2[f]=1\n",
        "\n",
        "print(arr2)\n",
        "#arr=p_resut"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bXZTEB8tCWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(dic)\n",
        "print(Cdic)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMOxKx1y65Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "for j,word in enumerate(t_words)\n",
        "  #string = \"I AM A CAT. As yet I have no name.\"\n",
        "  words = nltk.word_tokenize(t_string) #分かち書きをここで行っている→それぞれの単語をリスト化\n",
        "  wards=nltk.pos_tag(words) #品詞と単語の２次元リストで返す関数 ex)wards[x][y]→NNP\n",
        "  wards[1]\n",
        "  string = a_list[0]\n",
        "  #string = \"I AM A CAT. As yet I have no name.\"\n",
        "  words = nltk.word_tokenize(string) #分かち書きをここで行っている→それぞれの単語をリスト化\n",
        "  wards=nltk.pos_tag(words) #品詞と単語の２次元リストで返す関数 ex)wards[x][y]→NNP\n",
        "  wards[1]\n",
        "'''\n",
        "#https://www.haya-programming.com/entry/2018/03/21/234126\n",
        "#英語版の形態素解析\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "string = \"I AM A CAT. As yet I have no name.\"\n",
        "words = nltk.word_tokenize(string)\n",
        "words\n",
        "wards=nltk.pos_tag(words)\n",
        "wards[1]\n",
        "例題\n",
        "4\n",
        "By 2030, ensure that all young people and the majority (both men and women) adults have literacy and basic computing skills.\n",
        "By 2030, education for sustainable development and sustainable lifestyles, human rights, gender equality, promotion of peace and non-violent culture, global citizenship, sustainable development of cultural diversity and culture Ensure that all learners have the knowledge and skills necessary to promote sustainable development through education in understanding their contributions.\n",
        "Establish and improve educational facilities that are child, disability and gender sensitive and provide a safe, non-violent, inclusive and effective learning environment for all.\n",
        "By 2020, developed countries such as vocational training, information and communication technology (ICT), technology, engineering and science programs for developing countries, especially least developed and small island developing countries, and African countries Significantly increase the number of higher education scholarships worldwide in other developing countries.\n",
        "\n",
        "5\n",
        "Eliminate all forms of discrimination against all women and girls everywhere.\n",
        "5.2\n",
        "Eliminate all forms of violence in public and private spaces against all women and girls, including human trafficking, sexual and other types of exploitation.\n",
        "5.3\n",
        "Eliminate all harmful practices such as minor marriage, early marriage, forced marriage and genital mutilation.\n",
        "5.4\n",
        "Recognize and evaluate unpaid childcare / nursing care and domestic labor through provision of public services, infrastructure and social security policies, and sharing of responsibilities among households and families according to the situation in each country.\n",
        "5.5\n",
        "Ensure full and effective women's participation and equal leadership opportunities at all levels of decision-making in politics, economy and public sector.\n",
        "5.6\n",
        "Ensure universal access to sexual and reproductive health and rights in accordance with the International Population and Development Council (ICPD) Action Plan and Beijing Code of Conduct, and the outcome documents of these verification meetings.\n",
        "\n",
        "6\n",
        "By 2030, achieve universal and equal access to safe and affordable drinking water for all\n",
        "6.2\n",
        "By 2030, ensure access to adequate and equitable sewage and sanitation for all people and eliminate open defecation Pay particular attention to the needs of women and girls and vulnerable people.\n",
        "6.3\n",
        "By 2030, reducing pollution, eliminating dumping and minimizing the release of hazardous chemicals and substances, halving the proportion of untreated wastewater, and significantly increasing recycling and safe reuse on a global scale To improve water quality.\n",
        "6.4\n",
        "By 2030, drastically improve water use efficiency in all sectors, ensure sustainable collection and supply of fresh water, address water shortages, and significantly reduce the number of people suffering from water shortages.\n",
        "6.5\n",
        "By 2030, implement integrated water resources management at all levels, including through transboundary cooperation as appropriate.\n",
        "6.6\n",
        "By 2020, protect and restore water-related ecosystems, including mountains, forests, wetlands, rivers, aquifers, and lakes.\n",
        "\n",
        "#https://qiita.com/shimayu22/items/4fc6d01a4bf0eef909a0 参考記事\n",
        "#https://qiita.com/shimayu22/items/4fc6d01a4bf0eef909a0\n",
        "#課題～ハイパーパラメータをいじる、あるいはデータの抜き出し、整形の制度を上げる。\n",
        "#数値のベクトル化のところのアルゴリズムが少し不自然な気がするからそこの修正\n",
        "#ネット回線ないとコード動きません。\n",
        "#ネットのSDGs用語も辞書に足し合わせるべし←ネットに乗っている用語は重要度を上げるなどの改良の余地あり。\n",
        "#大文字を小文字にする処理や記号を抜き取る処理を追加する。\n",
        "import os\n",
        "import csv\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('punkt') #nltkを使うためのソフト群\n",
        "nltk.download('tagsets')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "csv_file = open(data_dir,'r') #学習データ\n",
        "p_csv_file= open(predict_dir,\"r\") #予測したいデータ\n",
        "\"\"\"\n",
        "csvファイルは左から目標番号、タイトル、アブスト、結論の順で入れること。\n",
        "\"\"\"\n",
        "p_list,t_list,a_list=[],[],[]\n",
        "#c_list= []\n",
        "\n",
        "delete= [\"a\",\"%\",\"of\",\"in\",\"on\",\"to\",\"for\",\"are\",\"at\"] #関連性のない語を集めた配列\n",
        "\n",
        "p_n_list=[]#予測したいデータのテキストバッグ\n",
        "\n",
        "for row in csv.reader(p_csv_file):\n",
        "  p_n_list.append(row[1])\n",
        "  p_n_list.append(row[2])\n",
        "\n",
        "x, y = [], []\n",
        "count = 0\n",
        "\n",
        "for row in csv.reader(csv_file):\n",
        "  #以下リストを作成\n",
        "    p_list.append(row[0]) #取得したい列番号を指定（0始まり）#目標番号をすべて取得\n",
        "    t_list.append(row[1]) #タイトルをすべて取得\n",
        "    a_list.append(row[2]) #アブストをすべて取得\n",
        "    #c_list.append(row[3]) #結論も入れる\n",
        "\n",
        "del p_list[0] #0番目を削除\n",
        "del t_list[0] #0番目は削除\n",
        "del a_list[0] #0番目は削除。これで_list[i]でそれぞれの列を抜き出すことができる。\n",
        "#del c_list[0]\n",
        "\n",
        "#以下３行　予測したいデータの形態素解析\n",
        "p_n_list=p_n_list[0]+p_n_list[1] \n",
        "p_n_words = nltk.word_tokenize(p_n_list)\n",
        "p_n_wards=nltk.pos_tag(p_n_words)#[(\"単語\",\"品詞\")....]の２次元リストをここで生成\n",
        "\n",
        "\n",
        "p_result = [] #名詞の数字配列を入れるp_resultを定義\n",
        "p_N_dic = [] #名詞のstring型配列を定義\n",
        "#以下名詞を抜き出しN_dicに入れ込み\n",
        "for word,part in p_n_wards: #wordに単語、partに品詞が入っている→このforはタイトル\n",
        "    if part==\"NN\" or part==\"NNP\" or part==\"VBP\": #以下if文は名詞のみをN_dicというリストに格納\n",
        "      if not word.lower() in delete: #さらにdeleteリストに入っている意味のない名詞を削除\n",
        "        sword=word.lower()\n",
        "        p_N_dic.append(sword)\n",
        "\n",
        "dic={} #辞書で定義\n",
        "for i,p in enumerate(p_list): #p_listのiをカウンタ変数,ｐを正解ラベル\n",
        "  j=i+1 #p_list,a_listなどは0番目が存在しないため無理やりjをカウンター変数に置き換え\n",
        "  if j>=len(p_list): #+1してるから最後の一つだけ存在しないjが入るためここで分岐処理　t_list[j_max]は存在しないためエラー\n",
        "    break\n",
        "  N_dic=[] #ここで名詞リストは初期化\n",
        "  #print(t_list[i])\n",
        "  t_string = t_list[j]\n",
        "  t_words = nltk.word_tokenize(t_string)\n",
        "  t_wards=nltk.pos_tag(t_words)#[(\"単語\",\"品詞\")....]の２次元リストをここで生成\n",
        "  #print(t_wards)\n",
        "  a_string = a_list[j]\n",
        "  a_words = nltk.word_tokenize(a_string)\n",
        "  a_wards=nltk.pos_tag(a_words)\n",
        "  \"\"\"\n",
        "  c_string = c_list[j]\n",
        "  c_words = nltk.word_tokenize(c_string)\n",
        "  c_wards=nltk.pos_tag(c_words)\n",
        "  \"\"\"\n",
        "  for word,part in t_wards: #wordに単語、partに品詞が入っている→このforはタイトル\n",
        "    if part==\"NN\" or part==\"NNP\" or part==\"VBP\": #以下if文は名詞のみをN_dicというリストに格納\n",
        "      if not word.lower() in delete: #さらにdeleteリストに入っている意味のない名詞を削除\n",
        "        sword=word.lower() #小文字に変換\n",
        "        N_dic.append(sword)\n",
        "  for word,part in a_wards: #wordに単語、partに品詞が入っている→このforはアブスト\n",
        "    if part==\"NN\" or part==\"NNP\" or part==\"VBP\": #以下if文は名詞のみをN_dicというリストに格納\n",
        "      if not word.lower() in delete:\n",
        "        sword=word.lower()\n",
        "        N_dic.append(sword)  \n",
        "  \"\"\"\n",
        "  for word,part in c_wards: #wordに単語、partに品詞が入っている→このforはアブスト\n",
        "    if part==\"NN\": #以下if文は名詞のみをN_dicというリストに格納\n",
        "      N_dic.append(word)\n",
        "    if part==\"NNP\":\n",
        "      N_dic.append(word)\n",
        "  \"\"\"\n",
        "  print(N_dic)#すべてN_dicにまとめてしまう。\n",
        "\n",
        "  result=[]\n",
        "  for word in N_dic:\n",
        "    word = word.strip()  \n",
        "    if word == \"\": continue\n",
        "    if not word in dic: # 未登録の場合\n",
        "      dic[word] = count  # count の数字で辞書に登録　補足：辞書型はwordとkeyの二つの情報を入れ込むことができる.これでNNに入れれるようにしていく\n",
        "      num = count\n",
        "      count +=1\n",
        "    #print(num,word)  # 数字と単語を表示\n",
        "    else:\n",
        "      num=dic[word] # 数字を辞書で調べる\n",
        "    result.append(num)  # リストに数字を追加 ここで名詞の塊を数値化している\n",
        "  x.append(result)  # リストを配列 ｘ に追加 x = [ [ 10, 11, 12, 3, 8, 13,  … 38 ] , [ 39, 40, 12, 16, 19, … ,56 ] , ….  ,  [ 1200, 1504, 3, 15, … 3300 ] ] \n",
        "  p_p=int(p)  #pは文字型で入っているのでキャスト\n",
        "  y.append(p_p)  # 正解ラベルを配列 y に追加 y = [ 0, 0, 0, 0, … ,8, 8, 8, 8 ] \n",
        "\n",
        "#以下のfor 予測したいデータの名詞配列の数値化 or 辞書の生成\n",
        "for word in p_N_dic:\n",
        "    word = word.strip()  \n",
        "    if word == \"\": continue\n",
        "    if not word in dic: # 未登録の場合\n",
        "      dic[word] = count  # count の数字で辞書に登録　補足：辞書型はwordとkeyの二つの情報を入れ込むことができる.これでNNに入れれるようにしていく\n",
        "      num = count\n",
        "      count +=1\n",
        "    #print(num,word)  # 数字と単語を表示\n",
        "    else:\n",
        "      num=dic[word] # 数字を辞書で調べる\n",
        "    p_result.append(num)  # リストに数字を追加 ここで名詞の塊を数値化している\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX84h9OmmqYt",
        "colab_type": "code",
        "outputId": "1bb3a9b8-38e0-47de-98f1-1d0f171e876a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "string = \"By 2030, ensure that all young people and the majority (both men and women) adults have literacy and basic computing skills.\"\n",
        "words = nltk.word_tokenize(string)\n",
        "words\n",
        "wards=nltk.pos_tag(words)\n",
        "for i in range(len(wards)):\n",
        "  print(wards[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "('By', 'IN')\n",
            "('2030', 'CD')\n",
            "(',', ',')\n",
            "('ensure', 'VB')\n",
            "('that', 'IN')\n",
            "('all', 'DT')\n",
            "('young', 'JJ')\n",
            "('people', 'NNS')\n",
            "('and', 'CC')\n",
            "('the', 'DT')\n",
            "('majority', 'NN')\n",
            "('(', '(')\n",
            "('both', 'DT')\n",
            "('men', 'NNS')\n",
            "('and', 'CC')\n",
            "('women', 'NNS')\n",
            "(')', ')')\n",
            "('adults', 'NNS')\n",
            "('have', 'VBP')\n",
            "('literacy', 'NN')\n",
            "('and', 'CC')\n",
            "('basic', 'JJ')\n",
            "('computing', 'VBG')\n",
            "('skills', 'NNS')\n",
            "('.', '.')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI3RtAn0mrY0",
        "colab_type": "code",
        "outputId": "00c7b6e9-3ecb-480e-dff3-a23767d40377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "#推薦単語アルゴリズム検証\n",
        "from collections import Counter #カウントするメソッドを使うためのモジュール\n",
        "l=[\"apple\",\"orange\",\"orange\",\"banana\",\"apple\",\"lemon\",\"apple\",\"frape\"]\n",
        "G=Counter(Cdic[2]).most_common() #多い順に返してくれる。ここで型をリストに変えてくれる\n",
        "print(G)\n",
        "L=G[0]\n",
        "print(L)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('system', 26), ('nematodes', 26), ('compost', 24), ('sludge', 22), ('reactor', 18), ('water', 17), ('sewage', 17), ('study', 16), ('bacillus', 13), ('cultivation', 11), ('l', 11), ('field', 10), ('mushroom', 9), ('soil', 9), ('root', 9), ('disease', 9), ('flow', 9), ('exchange', 9), ('development', 8), ('button', 8), ('lotus', 8), ('dbd', 7), ('plasma', 7), ('color', 7), ('characteristics', 6), ('community', 6), ('order', 6), ('types', 6), ('gene', 6), ('production', 6), ('plant', 6), ('blood', 6), ('ldv', 6), ('wastewater', 6), ('abr', 6), ('o', 6), ('aquaria', 6), ('cost', 5), ('sensor', 5), ('control', 5), ('time', 5), ('have', 5), ('plants', 5), ('bacteria', 5), ('diseases', 5), ('day', 5), ('technology', 5), ('application', 5), ('groups', 5), ('days', 5), ('performance', 5), ('operation', 5), ('information', 5), ('level', 4), ('rice', 4), ('attempt', 4), ('japan', 4), ('treatment', 4), ('ratio', 4), ('mushrooms', 4), ('productivity', 4), ('composts', 4), ('condition', 4), ('analysis', 4), ('’', 4), ('h.', 4), ('toothbrush', 4), ('use', 4), ('velocity', 4), ('test', 4), ('fertilizer', 4), ('spp', 4), ('tank', 4), ('dhs', 4), ('effluent', 4), ('volume', 4), ('hrt', 4), ('removal', 4), ('nh', 4), ('-1', 4), ('h', 4), ('wheelchair', 4), ('iot', 3), ('fields', 3), ('research', 3), ('materials', 3), ('content', 3), ('contents', 3), ('compositions', 3), ('c/n', 3), ('times', 3), ('sequence', 3), ('surface', 3), ('cause', 3), ('method', 3), ('growth', 3), ('purpose', 3), ('rate', 3), ('liquid', 3), ('multipoint', 3), ('measurement', 3), ('vessel', 3), ('distribution', 3), ('cs-mldv', 3), ('manure', 3), ('phase', 3), ('+', 3), ('sponge', 3), ('industry', 3), ('protein', 3), ('degradation', 3), ('dhs-usb', 3), ('freshwater', 3), ('concentration', 3), ('substances', 3), ('positioning', 3), ('farming', 2), ('lot', 2), ('efforts', 2), ('utilization', 2), ('amino', 2), ('acids', 2), ('difference', 2), ('improvement', 2), ('count', 2), ('elucidation', 2), ('parasitic-nematodes', 2), ('mechanism', 2), ('fruits', 2), ('quality', 2), ('decrease', 2), ('hirschmanniella', 2), ('diversa', 2), ('imamuri', 2), ('antibiotics', 2), ('bt', 2), ('genus', 2), ('fertilization', 2), ('t', 2), ('potato', 2), ('planters', 2), ('soils', 2), ('addition', 2), ('structures', 2), ('discharge', 2), ('streptococcus', 2), ('mutans', 2), ('porphyromonas', 2), ('gingivalis', 2), ('caries', 2), ('periodontitis', 2), ('energy', 2), ('material', 2), ('gas', 2), ('temperature', 2), ('effect', 2), ('laser', 2), ('resolution', 2), ('l-mldv', 2), ('line', 2), ('mouse', 2), ('components', 2), ('processing', 2), ('potential', 2), ('remain', 2), ('threat', 2), ('approaches', 2), ('disinfection', 2), ('pests', 2), ('increase', 2), ('sequences', 2), ('rrna', 2), ('yield', 2), ('tanks', 2), ('patterns', 2), ('exploration', 2), ('resource', 2), ('type', 2), ('salt', 2), ('minerals', 2), ('fishes', 2), ('aquarium', 2), ('fish', 2), ('•', 2), ('hanging', 2), ('nitrogen', 2), ('recovery', 2), ('anaerobic', 2), ('baffled', 2), ('marine', 2), ('g-cod/l', 2), ('capability', 2), ('process', 2), ('column', 2), ('cod', 2), ('cod/l', 2), ('mg-n/l', 2), ('oil', 2), ('grease', 2), ('methane', 2), ('ozone', 2), ('usb', 2), ('accumulation', 2), ('no', 2), ('ozone-dhs-usb', 2), ('mg', 2), ('n', 2), ('±', 2), ('unit', 2), ('competition', 2), ('users', 2), ('gps', 2), ('work', 2), ('spread', 2), ('accident', 2), ('water-level', 1), ('paddy', 1), ('embedded', 1), ('number', 1), ('farmers', 1), ('futhermore', 1), ('techniques', 1), ('trol', 1), ('exprience', 1), ('situations', 1), ('people', 1), ('internet', 1), ('need', 1), ('waterlevel', 1), ('propose', 1), ('optimized', 1), ('tons', 1), ('construction', 1), ('accounts', 1), ('total', 1), ('group', 1), ('biomass', 1), ('oyster', 1), ('methods', 1), ('bacillales', 1), ('carbon/nitrogen', 1), ('proportion', 1), ('requirement', 1), ('medium', 1), ('chips', 1), ('shochu', 1), ('”', 1), ('distillery', 1), ('wastes', 1), ('bran', 1), ('months', 1), ('evaluate', 1), ('agar', 1), ('plate', 1), ('plan', 1), ('ways', 1), ('controlling', 1), ('non-parasitic', 1), ('predominated', 1), ('compos', 1), ('vegetable', 1), ('t.', 1), ('situation', 1), ('problems', 1), ('kokuhi', 1), ('spot', 1), ('skin', 1), ('citrus', 1), ('lead', 1), ('deterioration', 1), ('loss', 1), ('yen', 1), ('tokushima', 1), ('prefecture', 1), ('species', 1), ('cedar', 1), ('bark', 1), ('toxin', 1), ('microorganisms', 1), ('environment', 1), ('fertility', 1), ('besides', 1), ('expect', 1), ('attack', 1), ('replant', 1), ('problem', 1), ('melidogyne', 1), ('incognita', 1), ('mechanisms', 1), ('goals', 1), ('conditions', 1), ('effects', 1), ('component', 1), ('organisms', 1), ('structure', 1), ('investigate', 1), ('colony', 1), ('month', 1), ('perform', 1), ('tests', 1), ('r', 1), ('d', 1), ('atmospheric-pressure', 1), ('applications', 1), ('inactivation', 1), ('germs', 1), ('fungi', 1), ('spores', 1), ('viruses', 1), ('hemostasis', 1), ('barrier', 1), ('transition', 1), ('years', 1), ('etc', 1), ('decades', 1), ('frequency', 1), ('infection', 1), ('risk', 1), ('high-voltage', 1), ('electrode', 1), ('air', 1), ('room', 1), ('consumption', 1), ('decomposition-efficiency', 1), ('carmine', 1), ('sterilization', 1), ('doppler', 1), ('velocimetry', 1), ('image', 1), ('monitoring', 1), ('living', 1), ('light', 1), ('report', 1), ('cross', 1), ('case', 1), ('surgery', 1), ('foods', 1), ('cropping', 1), ('melasma', 1), ('‘', 1), ('yuzuhada', 1), ('e.g', 1), ('seed', 1), ('require', 1), ('damage', 1), ('way', 1), ('molds', 1), ('insecticide', 1), ('abundance', 1), ('products', 1), ('niche', 1), ('goal', 1), ('studies', 1), ('miseq', 1), ('illumina', 1), ('copy', 1), ('numbers', 1), ('fertilizers', 1), ('combined', 1), ('cow', 1), ('properties', 1), ('agaricus', 1), ('bisporus', 1), ('supplement', 1), ('potassium', 1), ('mycelia', 1), ('amount', 1), ('metals', 1), ('fruit', 1), ('bodies', 1), ('media', 1), ('bubbly', 1), ('simulations', 1), ('rectangular', 1), ('fishery', 1), ('science', 1), ('aerators', 1), ('understanding', 1), ('observation', 1), ('different', 1), ('aspect', 1), ('ratios', 1), ('ar', 1), ('simulation', 1), ('aeration', 1), ('shape', 1), ('survival', 1), ('aquaculture', 1), ('vegitables', 1), ('proposal', 1), ('space', 1), ('key', 1), ('technologies', 1), ('nitrfication-denitrification', 1), ('high', 1), ('food', 1), ('valuable', 1), ('seaweed', 1), ('the', 1), ('with', 1), ('lipid', 1), ('and', 1), ('rich', 1), ('industries', 1), ('sector', 1), ('countries', 1), ('strength', 1), ('salinity', 1), ('/l', 1), ('lipids', 1), ('pretreatments', 1), ('resources', 1), ('input', 1), ('high-concentration', 1), ('difficulties', 1), ('characteristic', 1), ('multi', 1), ('compartmentalization', 1), ('benefits', 1), ('phases', 1), ('include', 1), ('acidification', 1), ('methanogenesis', 1), ('pipe', 1), ('center', 1), ('nagaoka', 1), ('university', 1), ('columns', 1), ('recirculation', 1), ('retention', 1), ('feeding', 1), ('influent', 1), ('fishmeal', 1), ('ammonia', 1), ('hands', 1), ('pathway', 1), ('appearance', 1), ('scum', 1), ('conversion', 1), ('resistant', 1), ('biogas', 1), ('degrade', 1), ('aqurarium', 1), ('treated', 1), ('by', 1), ('capacity', 1), ('basis', 1), ('-n', 1), ('toxicity', 1), ('nitrification', 1), ('down-flow', 1), ('denitrification', 1), ('up-', 1), ('blanket', 1), ('ammonium', 1), ('nitrate', 1), ('necessity', 1), ('due', 1), ('obligatory', 1), ('fatality', 1), ('animals', 1), ('combination', 1), ('presence', 1), ('aim', 1), ('suitability', 1), ('s', 1), ('standard', 1), ('term', 1), ('beauty', 1), ('site', 1), ('ph', 1), ('ozonation', 1), ('period', 1), ('carps', 1), ('cyprinus', 1), ('carpio', 1), ('units', 1), ('exposure', 1), ('re-application', 1), ('reading', 1), ('reduction', 1), ('outdoor', 1), ('driving', 1), ('geolocation', 1), ('map', 1), ('describes', 1), ('facilities', 1), ('route', 1), ('wheelchairs', 1), ('tokyo', 1), ('paralympic', 1), ('games', 1), ('troubles', 1), ('accidents', 1), ('venue', 1), ('advances', 1), ('location', 1), ('country', 1), ('michibiki', 1), ('aerospace', 1), ('agency', 1), ('jaxa', 1), ('communication', 1), ('ict', 1), ('prototype', 1), ('microcontroller', 1), ('board', 1), ('acceleration', 1)]\n",
            "('system', 26)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIxZFUxgTG-L",
        "colab_type": "code",
        "outputId": "2f1ff9ac-2dc1-45e9-ced0-38faa484c9dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "a=[]\n",
        "d=[\"dsa\",\"ads\",\"ww\"]\n",
        "b=[]\n",
        "c=[]\n",
        "c.append(a)\n",
        "c.append(b)\n",
        "c[1].extend(d)\n",
        "c\n",
        "for i range(16)\n",
        "  a=[]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[], ['dsa', 'ads', 'ww']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uotZ9_PdW7Bp",
        "colab_type": "code",
        "outputId": "b487ea51-0133-4520-8707-17abc5ef0ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "c=[]\n",
        "for i in range(16):\n",
        "  a=[]\n",
        "  c.append(a)\n",
        "d=\"wdsw\"\n",
        "ss=[\"ds\",\"dsss\",\"daas\"]\n",
        "c[4].extend([d])\n",
        "c[4].extend(ss)\n",
        "print(c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[], [], [], [], ['wdsw', 'ds', 'dsss', 'daas'], [], [], [], [], [], [], [], [], [], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKzFOLmJaBfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def counts(a,b):\n",
        "  print(\"aaa\"+str(a))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbl74KItncK2",
        "colab_type": "code",
        "outputId": "f008c934-48bd-46ef-af3d-ced62a0c2950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "counts(1,1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aaa1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-mvZdhEneak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}